{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///DisasterResponse.db')\n",
    "df = pd.read_sql('SELECT * FROM message', engine)\n",
    "X = df.message\n",
    "y = df.iloc[:, 4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "pipeline = Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', MultiOutputClassifier(KNeighborsClassifier())),\n",
    "                ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size =.3,random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def get_scores(y_pred,y_test):\n",
    "    result = []\n",
    "    for i in range(y_test.shape[1]): \n",
    "        test_value = y_test.iloc[:, i]\n",
    "        pred_value = [a[i] for a in y_pred]\n",
    "        result.append(list(classification_report(test_value, pred_value,output_dict = True)['0'].values())[:3])\n",
    "    return pd.DataFrame(result,columns=['precision','recall','f1_score']).mean()\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "precision    0.931183\n",
       "recall       0.973001\n",
       "f1_score     0.950552\n",
       "dtype: float64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNeighborsClassifier_score = get_scores(y_pred,y_test)\n",
    "KNeighborsClassifier_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function tokenize at 0x1a1cd2d290>, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=KNeighborsClassifier(algorithm='auto',\n",
       "                                                        leaf_size=30,\n",
       "                                                        metric='minkowski',\n",
       "                                                        metric_params=None,\n",
       "                                                        n_jobs=None, n_neighbors=5,\n",
       "                                                        p=2, weights='uniform'),\n",
       "                         n_jobs=None))],\n",
       " 'verbose': False,\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function tokenize at 0x1a1cd2d290>, vocabulary=None),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'clf': MultiOutputClassifier(estimator=KNeighborsClassifier(algorithm='auto',\n",
       "                                                      leaf_size=30,\n",
       "                                                      metric='minkowski',\n",
       "                                                      metric_params=None,\n",
       "                                                      n_jobs=None, n_neighbors=5,\n",
       "                                                      p=2, weights='uniform'),\n",
       "                       n_jobs=None),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__algorithm': 'auto',\n",
       " 'clf__estimator__leaf_size': 30,\n",
       " 'clf__estimator__metric': 'minkowski',\n",
       " 'clf__estimator__metric_params': None,\n",
       " 'clf__estimator__n_jobs': None,\n",
       " 'clf__estimator__n_neighbors': 5,\n",
       " 'clf__estimator__p': 2,\n",
       " 'clf__estimator__weights': 'uniform',\n",
       " 'clf__estimator': KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                      metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                      weights='uniform'),\n",
       " 'clf__n_jobs': None}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', MultiOutputClassifier(KNeighborsClassifier())),\n",
    "                ])\n",
    "\n",
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = { \n",
    "    'vect__max_df':[0.5,1.0],\n",
    "    'clf__estimator__n_neighbors':[3,5,7],\n",
    "    'clf__estimator__leaf_size':[20,30,40],    \n",
    "    }\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters, cv = 2, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the train data\n",
    "cv.fit(X_train, y_train)\n",
    "# get the pred value\n",
    "y_pred = cv.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([4.54942596, 4.61293054, 3.07861483, 3.14130759, 3.50891495,\n",
       "        3.49740267, 3.32925618, 3.30847251, 3.14226794, 3.09994531,\n",
       "        3.33424139, 3.28952599, 3.11634195, 3.47763455, 3.52636051,\n",
       "        3.34596908, 3.30294824, 3.3062259 ]),\n",
       " 'std_fit_time': array([0.10222399, 0.04877734, 0.04008615, 0.04157448, 0.02964711,\n",
       "        0.1558845 , 0.03394496, 0.03111446, 0.03133202, 0.02516842,\n",
       "        0.03896141, 0.00840712, 0.04161489, 0.05200255, 0.09935045,\n",
       "        0.07057893, 0.02718258, 0.06511796]),\n",
       " 'mean_score_time': array([137.42053699, 160.08084941, 146.95691717, 166.46277857,\n",
       "        156.90371966, 176.8538239 , 124.36755335, 145.87705302,\n",
       "        150.89438033, 165.57653749, 142.56114793, 200.483881  ,\n",
       "        159.44192851, 183.32705736, 183.17842948, 171.36719728,\n",
       "        154.1842221 , 134.82823122]),\n",
       " 'std_score_time': array([0.14189005, 0.04880738, 1.32731521, 1.90153146, 2.95004249,\n",
       "        1.12354803, 0.50455534, 0.32984805, 5.61507058, 1.49859846,\n",
       "        1.69693589, 3.80265808, 3.0756644 , 2.29858351, 1.30858958,\n",
       "        4.88954449, 4.15358698, 2.56503189]),\n",
       " 'param_clf__estimator__leaf_size': masked_array(data=[20, 20, 20, 20, 20, 20, 30, 30, 30, 30, 30, 30, 40, 40,\n",
       "                    40, 40, 40, 40],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_clf__estimator__n_neighbors': masked_array(data=[3, 3, 5, 5, 7, 7, 3, 3, 5, 5, 7, 7, 3, 3, 5, 5, 7, 7],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vect__max_df': masked_array(data=[0.5, 1.0, 0.5, 1.0, 0.5, 1.0, 0.5, 1.0, 0.5, 1.0, 0.5,\n",
       "                    1.0, 0.5, 1.0, 0.5, 1.0, 0.5, 1.0],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'clf__estimator__leaf_size': 20,\n",
       "   'clf__estimator__n_neighbors': 3,\n",
       "   'vect__max_df': 0.5},\n",
       "  {'clf__estimator__leaf_size': 20,\n",
       "   'clf__estimator__n_neighbors': 3,\n",
       "   'vect__max_df': 1.0},\n",
       "  {'clf__estimator__leaf_size': 20,\n",
       "   'clf__estimator__n_neighbors': 5,\n",
       "   'vect__max_df': 0.5},\n",
       "  {'clf__estimator__leaf_size': 20,\n",
       "   'clf__estimator__n_neighbors': 5,\n",
       "   'vect__max_df': 1.0},\n",
       "  {'clf__estimator__leaf_size': 20,\n",
       "   'clf__estimator__n_neighbors': 7,\n",
       "   'vect__max_df': 0.5},\n",
       "  {'clf__estimator__leaf_size': 20,\n",
       "   'clf__estimator__n_neighbors': 7,\n",
       "   'vect__max_df': 1.0},\n",
       "  {'clf__estimator__leaf_size': 30,\n",
       "   'clf__estimator__n_neighbors': 3,\n",
       "   'vect__max_df': 0.5},\n",
       "  {'clf__estimator__leaf_size': 30,\n",
       "   'clf__estimator__n_neighbors': 3,\n",
       "   'vect__max_df': 1.0},\n",
       "  {'clf__estimator__leaf_size': 30,\n",
       "   'clf__estimator__n_neighbors': 5,\n",
       "   'vect__max_df': 0.5},\n",
       "  {'clf__estimator__leaf_size': 30,\n",
       "   'clf__estimator__n_neighbors': 5,\n",
       "   'vect__max_df': 1.0},\n",
       "  {'clf__estimator__leaf_size': 30,\n",
       "   'clf__estimator__n_neighbors': 7,\n",
       "   'vect__max_df': 0.5},\n",
       "  {'clf__estimator__leaf_size': 30,\n",
       "   'clf__estimator__n_neighbors': 7,\n",
       "   'vect__max_df': 1.0},\n",
       "  {'clf__estimator__leaf_size': 40,\n",
       "   'clf__estimator__n_neighbors': 3,\n",
       "   'vect__max_df': 0.5},\n",
       "  {'clf__estimator__leaf_size': 40,\n",
       "   'clf__estimator__n_neighbors': 3,\n",
       "   'vect__max_df': 1.0},\n",
       "  {'clf__estimator__leaf_size': 40,\n",
       "   'clf__estimator__n_neighbors': 5,\n",
       "   'vect__max_df': 0.5},\n",
       "  {'clf__estimator__leaf_size': 40,\n",
       "   'clf__estimator__n_neighbors': 5,\n",
       "   'vect__max_df': 1.0},\n",
       "  {'clf__estimator__leaf_size': 40,\n",
       "   'clf__estimator__n_neighbors': 7,\n",
       "   'vect__max_df': 0.5},\n",
       "  {'clf__estimator__leaf_size': 40,\n",
       "   'clf__estimator__n_neighbors': 7,\n",
       "   'vect__max_df': 1.0}],\n",
       " 'split0_test_score': array([0.21512642, 0.20738884, 0.21479948, 0.20095902, 0.21828684,\n",
       "        0.20357454, 0.21512642, 0.20738884, 0.21479948, 0.20095902,\n",
       "        0.21828684, 0.20357454, 0.21512642, 0.20738884, 0.21479948,\n",
       "        0.20095902, 0.21828684, 0.20357454]),\n",
       " 'split1_test_score': array([0.23771117, 0.23019074, 0.23923706, 0.22190736, 0.23117166,\n",
       "        0.2119891 , 0.23771117, 0.23019074, 0.23923706, 0.22190736,\n",
       "        0.23117166, 0.2119891 , 0.23771117, 0.23019074, 0.23923706,\n",
       "        0.22190736, 0.23117166, 0.2119891 ]),\n",
       " 'mean_test_score': array([0.22641879, 0.21878979, 0.22701827, 0.21143319, 0.22472925,\n",
       "        0.20778182, 0.22641879, 0.21878979, 0.22701827, 0.21143319,\n",
       "        0.22472925, 0.20778182, 0.22641879, 0.21878979, 0.22701827,\n",
       "        0.21143319, 0.22472925, 0.20778182]),\n",
       " 'std_test_score': array([0.01129238, 0.01140095, 0.01221879, 0.01047417, 0.00644241,\n",
       "        0.00420728, 0.01129238, 0.01140095, 0.01221879, 0.01047417,\n",
       "        0.00644241, 0.00420728, 0.01129238, 0.01140095, 0.01221879,\n",
       "        0.01047417, 0.00644241, 0.00420728]),\n",
       " 'rank_test_score': array([ 4, 10,  1, 13,  7, 16,  4, 10,  1, 13,  7, 16,  4, 10,  1, 13,  7,\n",
       "        16], dtype=int32)}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__leaf_size': 20,\n",
       " 'clf__estimator__n_neighbors': 5,\n",
       " 'vect__max_df': 0.5}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "precision    0.929722\n",
       "recall       0.978194\n",
       "f1_score     0.952168\n",
       "dtype: float64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the three scores of the best combination\n",
    "get_scores(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#first, try to add a text length feature and a starting verb feature\n",
    "import nltk\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TextLengthExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def textlength(self, text):        \n",
    "        return len(tokenize(text))\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_tagged = pd.Series(X).apply(self.textlength)\n",
    "        return pd.DataFrame(X_tagged)\n",
    "\n",
    "\n",
    "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def starting_verb(self, text):\n",
    "        sentence_list = nltk.sent_tokenize(text)\n",
    "        for sentence in sentence_list:\n",
    "            pos_tags = nltk.pos_tag(tokenize(sentence))\n",
    "            first_word, first_tag = pos_tags[0]\n",
    "            if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_tagged = pd.Series(X).apply(self.starting_verb)\n",
    "        return pd.DataFrame(X_tagged)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_union = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "\n",
    "        ('nlp_pipeline', Pipeline([\n",
    "            ('vect', CountVectorizer(tokenizer=tokenize,max_df =0.5)),\n",
    "            ('tfidf', TfidfTransformer())\n",
    "             ])),\n",
    "\n",
    "        ('txt_len', TextLengthExtractor()),\n",
    "        ('start_verb', StartingVerbExtractor())\n",
    "    ])),\n",
    "\n",
    "    ('clf', MultiOutputClassifier(KNeighborsClassifier(leaf_size =20)))\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipeline_union.fit(X_train, y_train)\n",
    "y_pred_union = pipeline_union.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "precision    0.934191\n",
       "recall       0.957524\n",
       "f1_score     0.944294\n",
       "dtype: float64"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_scores(y_pred_union,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try randomforestclassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipeline_rf = Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', MultiOutputClassifier(RandomForestClassifier())),\n",
    "                ])\n",
    "\n",
    "pipeline_rf.fit(X_train, y_train)\n",
    "y_pred_rf = pipeline_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "precision    0.944321\n",
       "recall       0.973485\n",
       "f1_score     0.955139\n",
       "dtype: float64"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_scores(y_pred_rf,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function tokenize at 0x1a1cd2d290>, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                          ccp_alpha=0.0,\n",
       "                                                          class_weight=None,\n",
       "                                                          criterion='gini',\n",
       "                                                          max_depth=None,\n",
       "                                                          max_features='auto',\n",
       "                                                          max_leaf_nodes=None,\n",
       "                                                          max_samples=None,\n",
       "                                                          min_impurity_decrease=0.0,\n",
       "                                                          min_impurity_split=None,\n",
       "                                                          min_samples_leaf=1,\n",
       "                                                          min_samples_split=2,\n",
       "                                                          min_weight_fraction_leaf=0.0,\n",
       "                                                          n_estimators=100,\n",
       "                                                          n_jobs=None,\n",
       "                                                          oob_score=False,\n",
       "                                                          random_state=None,\n",
       "                                                          verbose=0,\n",
       "                                                          warm_start=False),\n",
       "                         n_jobs=None))],\n",
       " 'verbose': False,\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function tokenize at 0x1a1cd2d290>, vocabulary=None),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'clf': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                        ccp_alpha=0.0,\n",
       "                                                        class_weight=None,\n",
       "                                                        criterion='gini',\n",
       "                                                        max_depth=None,\n",
       "                                                        max_features='auto',\n",
       "                                                        max_leaf_nodes=None,\n",
       "                                                        max_samples=None,\n",
       "                                                        min_impurity_decrease=0.0,\n",
       "                                                        min_impurity_split=None,\n",
       "                                                        min_samples_leaf=1,\n",
       "                                                        min_samples_split=2,\n",
       "                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                        n_estimators=100,\n",
       "                                                        n_jobs=None,\n",
       "                                                        oob_score=False,\n",
       "                                                        random_state=None,\n",
       "                                                        verbose=0,\n",
       "                                                        warm_start=False),\n",
       "                       n_jobs=None),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__bootstrap': True,\n",
       " 'clf__estimator__ccp_alpha': 0.0,\n",
       " 'clf__estimator__class_weight': None,\n",
       " 'clf__estimator__criterion': 'gini',\n",
       " 'clf__estimator__max_depth': None,\n",
       " 'clf__estimator__max_features': 'auto',\n",
       " 'clf__estimator__max_leaf_nodes': None,\n",
       " 'clf__estimator__max_samples': None,\n",
       " 'clf__estimator__min_impurity_decrease': 0.0,\n",
       " 'clf__estimator__min_impurity_split': None,\n",
       " 'clf__estimator__min_samples_leaf': 1,\n",
       " 'clf__estimator__min_samples_split': 2,\n",
       " 'clf__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__estimator__n_estimators': 100,\n",
       " 'clf__estimator__n_jobs': None,\n",
       " 'clf__estimator__oob_score': False,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator__verbose': 0,\n",
       " 'clf__estimator__warm_start': False,\n",
       " 'clf__estimator': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=None, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       " 'clf__n_jobs': None}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_rf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = { \n",
    "    'vect__max_df':[0.5,1.0],\n",
    "    'clf__estimator__min_samples_leaf':[1,10,100],\n",
    "    'clf__estimator__n_estimators':[10,100,1000],    \n",
    "    }\n",
    "\n",
    "cv_rf = GridSearchCV(pipeline_rf, param_grid=parameters, cv = 2, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuhao3/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    }
   ],
   "source": [
    "# fit the train data\n",
    "cv_rf.fit(X_train, y_train)\n",
    "# get the pred value\n",
    "y_pred_rf = cv_rf.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([  14.9142375 ,   15.11135793,  103.81421053,  105.32928455,\n",
       "        1050.12543797, 1077.2287184 ,    6.2204361 ,    6.08525693,\n",
       "          31.27337515,   31.13991296,  271.02321637,  271.62071645,\n",
       "           5.26762342,    5.27831447,   10.80115867,   10.93074942,\n",
       "          81.16829395,   82.49217212]),\n",
       " 'std_fit_time': array([1.97382450e-01, 2.30489016e-01, 2.47808564e+00, 1.98966563e+00,\n",
       "        2.72807801e+01, 3.34113194e+01, 1.36315107e-01, 1.67208910e-02,\n",
       "        6.11735940e-01, 6.28281832e-02, 8.87270570e-01, 5.10180593e-01,\n",
       "        3.50325108e-02, 4.67332602e-02, 6.94355965e-02, 4.04474735e-02,\n",
       "        1.15309000e-01, 1.98697925e-01]),\n",
       " 'mean_score_time': array([  5.28376389,   5.20149052,  27.16379309,  26.39425647,\n",
       "        560.57951701, 534.6114856 ,   4.24459434,   4.09834063,\n",
       "         13.91008496,  13.09914851, 107.33869994, 100.79286802,\n",
       "          3.21042144,   3.19971061,   5.97512281,   6.18708014,\n",
       "         31.39809954,  32.22653484]),\n",
       " 'std_score_time': array([8.25719833e-02, 5.79476357e-04, 3.18638086e-01, 3.49213719e-01,\n",
       "        2.01805011e+01, 2.94166166e+01, 9.76486206e-02, 5.74506521e-02,\n",
       "        1.87471867e-01, 1.21564865e-02, 2.25302815e-01, 1.58861995e-01,\n",
       "        3.50116491e-02, 4.67333794e-02, 1.45059824e-02, 5.57088852e-03,\n",
       "        8.65815878e-02, 8.08990002e-02]),\n",
       " 'param_clf__estimator__min_samples_leaf': masked_array(data=[1, 1, 1, 1, 1, 1, 10, 10, 10, 10, 10, 10, 100, 100,\n",
       "                    100, 100, 100, 100],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_clf__estimator__n_estimators': masked_array(data=[10, 10, 100, 100, 1000, 1000, 10, 10, 100, 100, 1000,\n",
       "                    1000, 10, 10, 100, 100, 1000, 1000],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vect__max_df': masked_array(data=[0.5, 1.0, 0.5, 1.0, 0.5, 1.0, 0.5, 1.0, 0.5, 1.0, 0.5,\n",
       "                    1.0, 0.5, 1.0, 0.5, 1.0, 0.5, 1.0],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'clf__estimator__min_samples_leaf': 1,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'vect__max_df': 0.5},\n",
       "  {'clf__estimator__min_samples_leaf': 1,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'vect__max_df': 1.0},\n",
       "  {'clf__estimator__min_samples_leaf': 1,\n",
       "   'clf__estimator__n_estimators': 100,\n",
       "   'vect__max_df': 0.5},\n",
       "  {'clf__estimator__min_samples_leaf': 1,\n",
       "   'clf__estimator__n_estimators': 100,\n",
       "   'vect__max_df': 1.0},\n",
       "  {'clf__estimator__min_samples_leaf': 1,\n",
       "   'clf__estimator__n_estimators': 1000,\n",
       "   'vect__max_df': 0.5},\n",
       "  {'clf__estimator__min_samples_leaf': 1,\n",
       "   'clf__estimator__n_estimators': 1000,\n",
       "   'vect__max_df': 1.0},\n",
       "  {'clf__estimator__min_samples_leaf': 10,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'vect__max_df': 0.5},\n",
       "  {'clf__estimator__min_samples_leaf': 10,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'vect__max_df': 1.0},\n",
       "  {'clf__estimator__min_samples_leaf': 10,\n",
       "   'clf__estimator__n_estimators': 100,\n",
       "   'vect__max_df': 0.5},\n",
       "  {'clf__estimator__min_samples_leaf': 10,\n",
       "   'clf__estimator__n_estimators': 100,\n",
       "   'vect__max_df': 1.0},\n",
       "  {'clf__estimator__min_samples_leaf': 10,\n",
       "   'clf__estimator__n_estimators': 1000,\n",
       "   'vect__max_df': 0.5},\n",
       "  {'clf__estimator__min_samples_leaf': 10,\n",
       "   'clf__estimator__n_estimators': 1000,\n",
       "   'vect__max_df': 1.0},\n",
       "  {'clf__estimator__min_samples_leaf': 100,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'vect__max_df': 0.5},\n",
       "  {'clf__estimator__min_samples_leaf': 100,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'vect__max_df': 1.0},\n",
       "  {'clf__estimator__min_samples_leaf': 100,\n",
       "   'clf__estimator__n_estimators': 100,\n",
       "   'vect__max_df': 0.5},\n",
       "  {'clf__estimator__min_samples_leaf': 100,\n",
       "   'clf__estimator__n_estimators': 100,\n",
       "   'vect__max_df': 1.0},\n",
       "  {'clf__estimator__min_samples_leaf': 100,\n",
       "   'clf__estimator__n_estimators': 1000,\n",
       "   'vect__max_df': 0.5},\n",
       "  {'clf__estimator__min_samples_leaf': 100,\n",
       "   'clf__estimator__n_estimators': 1000,\n",
       "   'vect__max_df': 1.0}],\n",
       " 'split0_test_score': array([0.21861378, 0.21436356, 0.23768527, 0.23812119, 0.23866609,\n",
       "        0.23659547, 0.17632956, 0.1728422 , 0.18068875, 0.17676548,\n",
       "        0.18112467, 0.17850915, 0.19387533, 0.19387533, 0.19387533,\n",
       "        0.19387533, 0.19387533, 0.19387533]),\n",
       " 'split1_test_score': array([0.22288828, 0.21569482, 0.23607629, 0.23269755, 0.23923706,\n",
       "        0.23411444, 0.19029973, 0.1653406 , 0.18147139, 0.17700272,\n",
       "        0.17831063, 0.17558583, 0.19858311, 0.19858311, 0.19858311,\n",
       "        0.19858311, 0.19858311, 0.19858311]),\n",
       " 'mean_test_score': array([0.22075103, 0.21502919, 0.23688078, 0.23540937, 0.23895157,\n",
       "        0.23535495, 0.18331464, 0.1690914 , 0.18108007, 0.1768841 ,\n",
       "        0.17971765, 0.17704749, 0.19622922, 0.19622922, 0.19622922,\n",
       "        0.19622922, 0.19622922, 0.19622922]),\n",
       " 'std_test_score': array([0.00213725, 0.00066563, 0.00080449, 0.00271182, 0.00028549,\n",
       "        0.00124051, 0.00698509, 0.0037508 , 0.00039132, 0.00011862,\n",
       "        0.00140702, 0.00146166, 0.00235389, 0.00235389, 0.00235389,\n",
       "        0.00235389, 0.00235389, 0.00235389]),\n",
       " 'rank_test_score': array([ 5,  6,  2,  3,  1,  4, 13, 18, 14, 17, 15, 16,  7,  7,  7,  7,  7,\n",
       "         7], dtype=int32)}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_rf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__min_samples_leaf': 1,\n",
       " 'clf__estimator__n_estimators': 1000,\n",
       " 'vect__max_df': 0.5}"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "precision    0.945068\n",
       "recall       0.973044\n",
       "f1_score     0.955248\n",
       "dtype: float64"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_scores(y_pred_rf,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_union2 = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "\n",
    "        ('nlp_pipeline', Pipeline([\n",
    "            ('vect', CountVectorizer(tokenizer=tokenize, max_df = 0.5)),\n",
    "            ('tfidf', TfidfTransformer())\n",
    "             ])),\n",
    "\n",
    "        ('txt_len', TextLengthExtractor()),\n",
    "        ('start_verb', StartingVerbExtractor())\n",
    "    ])),\n",
    "\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(min_samples_leaf =1,n_estimators = 1000)))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = time.time()\n",
    "pipeline_union2.fit(X_train, y_train)\n",
    "y_pred_union2 = pipeline_union2.predict(X_test)\n",
    "time_cost = time.time() - now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "precision    0.945194\n",
       "recall       0.972778\n",
       "f1_score     0.955031\n",
       "dtype: float64"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_scores(y_pred_union2,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3095.8152940273285"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# try naive bayes classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "pipeline_nb = Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('todense',FunctionTransformer(lambda x: x.todense(), accept_sparse=True)), \n",
    "                ('clf', MultiOutputClassifier(GaussianNB())),\n",
    "                ])\n",
    "\n",
    "pipeline_nb.fit(X_train, y_train)\n",
    "y_pred_nb = pipeline_nb.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "precision    0.929827\n",
       "recall       0.778979\n",
       "f1_score     0.842159\n",
       "dtype: float64"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_scores(y_pred_nb,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00014901161193847656"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "a = time.time()\n",
    "a\n",
    "for i in range(1000):\n",
    "    i += 1\n",
    "b = time.time() - a\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_randomfo = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "\n",
    "        ('nlp_pipeline', Pipeline([\n",
    "            ('vect', CountVectorizer(tokenizer=tokenize, max_df = 0.5)),\n",
    "            ('tfidf', TfidfTransformer())\n",
    "             ])),\n",
    "\n",
    "        ('txt_len', TextLengthExtractor()),\n",
    "        ('start_verb', StartingVerbExtractor())\n",
    "    ])),\n",
    "\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(min_samples_leaf =1,n_estimators = 1000)))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('features', FeatureUnion(n_jobs=None,\n",
       "                transformer_list=[('nlp_pipeline',\n",
       "                                   Pipeline(memory=None,\n",
       "                                            steps=[('vect',\n",
       "                                                    CountVectorizer(analyzer='word',\n",
       "                                                                    binary=False,\n",
       "                                                                    decode_error='strict',\n",
       "                                                                    dtype=<class 'numpy.int64'>,\n",
       "                                                                    encoding='utf-8',\n",
       "                                                                    input='content',\n",
       "                                                                    lowercase=True,\n",
       "                                                                    max_df=0.5,\n",
       "                                                                    max_features=None,\n",
       "                                                                    min_df=1,\n",
       "                                                                    ngram_range=(1,\n",
       "                                                                                 1),\n",
       "                                                                    preprocessor=None,\n",
       "                                                                    stop_words=None,\n",
       "                                                                    strip_accents=None,\n",
       "                                                                    token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                                    tokenizer=<function tokenize at 0x1a1cd2d290>,\n",
       "                                                                    vocabulary=None)),\n",
       "                                                   ('tfidf',\n",
       "                                                    TfidfTransformer(norm='l2',\n",
       "                                                                     smooth_idf=True,\n",
       "                                                                     sublinear_tf=False,\n",
       "                                                                     use_idf=True))],\n",
       "                                            verbose=False)),\n",
       "                                  ('txt_len', TextLengthExtractor()),\n",
       "                                  ('start_verb', StartingVerbExtractor())],\n",
       "                transformer_weights=None, verbose=False)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                          ccp_alpha=0.0,\n",
       "                                                          class_weight=None,\n",
       "                                                          criterion='gini',\n",
       "                                                          max_depth=None,\n",
       "                                                          max_features='auto',\n",
       "                                                          max_leaf_nodes=None,\n",
       "                                                          max_samples=None,\n",
       "                                                          min_impurity_decrease=0.0,\n",
       "                                                          min_impurity_split=None,\n",
       "                                                          min_samples_leaf=1,\n",
       "                                                          min_samples_split=2,\n",
       "                                                          min_weight_fraction_leaf=0.0,\n",
       "                                                          n_estimators=1000,\n",
       "                                                          n_jobs=None,\n",
       "                                                          oob_score=False,\n",
       "                                                          random_state=None,\n",
       "                                                          verbose=0,\n",
       "                                                          warm_start=False),\n",
       "                         n_jobs=None))],\n",
       " 'verbose': False,\n",
       " 'features': FeatureUnion(n_jobs=None,\n",
       "              transformer_list=[('nlp_pipeline',\n",
       "                                 Pipeline(memory=None,\n",
       "                                          steps=[('vect',\n",
       "                                                  CountVectorizer(analyzer='word',\n",
       "                                                                  binary=False,\n",
       "                                                                  decode_error='strict',\n",
       "                                                                  dtype=<class 'numpy.int64'>,\n",
       "                                                                  encoding='utf-8',\n",
       "                                                                  input='content',\n",
       "                                                                  lowercase=True,\n",
       "                                                                  max_df=0.5,\n",
       "                                                                  max_features=None,\n",
       "                                                                  min_df=1,\n",
       "                                                                  ngram_range=(1,\n",
       "                                                                               1),\n",
       "                                                                  preprocessor=None,\n",
       "                                                                  stop_words=None,\n",
       "                                                                  strip_accents=None,\n",
       "                                                                  token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                                  tokenizer=<function tokenize at 0x1a1cd2d290>,\n",
       "                                                                  vocabulary=None)),\n",
       "                                                 ('tfidf',\n",
       "                                                  TfidfTransformer(norm='l2',\n",
       "                                                                   smooth_idf=True,\n",
       "                                                                   sublinear_tf=False,\n",
       "                                                                   use_idf=True))],\n",
       "                                          verbose=False)),\n",
       "                                ('txt_len', TextLengthExtractor()),\n",
       "                                ('start_verb', StartingVerbExtractor())],\n",
       "              transformer_weights=None, verbose=False),\n",
       " 'clf': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                        ccp_alpha=0.0,\n",
       "                                                        class_weight=None,\n",
       "                                                        criterion='gini',\n",
       "                                                        max_depth=None,\n",
       "                                                        max_features='auto',\n",
       "                                                        max_leaf_nodes=None,\n",
       "                                                        max_samples=None,\n",
       "                                                        min_impurity_decrease=0.0,\n",
       "                                                        min_impurity_split=None,\n",
       "                                                        min_samples_leaf=1,\n",
       "                                                        min_samples_split=2,\n",
       "                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                        n_estimators=1000,\n",
       "                                                        n_jobs=None,\n",
       "                                                        oob_score=False,\n",
       "                                                        random_state=None,\n",
       "                                                        verbose=0,\n",
       "                                                        warm_start=False),\n",
       "                       n_jobs=None),\n",
       " 'features__n_jobs': None,\n",
       " 'features__transformer_list': [('nlp_pipeline',\n",
       "   Pipeline(memory=None,\n",
       "            steps=[('vect',\n",
       "                    CountVectorizer(analyzer='word', binary=False,\n",
       "                                    decode_error='strict',\n",
       "                                    dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                    input='content', lowercase=True, max_df=0.5,\n",
       "                                    max_features=None, min_df=1,\n",
       "                                    ngram_range=(1, 1), preprocessor=None,\n",
       "                                    stop_words=None, strip_accents=None,\n",
       "                                    token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                    tokenizer=<function tokenize at 0x1a1cd2d290>,\n",
       "                                    vocabulary=None)),\n",
       "                   ('tfidf',\n",
       "                    TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                     sublinear_tf=False, use_idf=True))],\n",
       "            verbose=False)),\n",
       "  ('txt_len', TextLengthExtractor()),\n",
       "  ('start_verb', StartingVerbExtractor())],\n",
       " 'features__transformer_weights': None,\n",
       " 'features__verbose': False,\n",
       " 'features__nlp_pipeline': Pipeline(memory=None,\n",
       "          steps=[('vect',\n",
       "                  CountVectorizer(analyzer='word', binary=False,\n",
       "                                  decode_error='strict',\n",
       "                                  dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                  input='content', lowercase=True, max_df=0.5,\n",
       "                                  max_features=None, min_df=1,\n",
       "                                  ngram_range=(1, 1), preprocessor=None,\n",
       "                                  stop_words=None, strip_accents=None,\n",
       "                                  token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                  tokenizer=<function tokenize at 0x1a1cd2d290>,\n",
       "                                  vocabulary=None)),\n",
       "                 ('tfidf',\n",
       "                  TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                   sublinear_tf=False, use_idf=True))],\n",
       "          verbose=False),\n",
       " 'features__txt_len': TextLengthExtractor(),\n",
       " 'features__start_verb': StartingVerbExtractor(),\n",
       " 'features__nlp_pipeline__memory': None,\n",
       " 'features__nlp_pipeline__steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function tokenize at 0x1a1cd2d290>, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))],\n",
       " 'features__nlp_pipeline__verbose': False,\n",
       " 'features__nlp_pipeline__vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function tokenize at 0x1a1cd2d290>, vocabulary=None),\n",
       " 'features__nlp_pipeline__tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'features__nlp_pipeline__vect__analyzer': 'word',\n",
       " 'features__nlp_pipeline__vect__binary': False,\n",
       " 'features__nlp_pipeline__vect__decode_error': 'strict',\n",
       " 'features__nlp_pipeline__vect__dtype': numpy.int64,\n",
       " 'features__nlp_pipeline__vect__encoding': 'utf-8',\n",
       " 'features__nlp_pipeline__vect__input': 'content',\n",
       " 'features__nlp_pipeline__vect__lowercase': True,\n",
       " 'features__nlp_pipeline__vect__max_df': 0.5,\n",
       " 'features__nlp_pipeline__vect__max_features': None,\n",
       " 'features__nlp_pipeline__vect__min_df': 1,\n",
       " 'features__nlp_pipeline__vect__ngram_range': (1, 1),\n",
       " 'features__nlp_pipeline__vect__preprocessor': None,\n",
       " 'features__nlp_pipeline__vect__stop_words': None,\n",
       " 'features__nlp_pipeline__vect__strip_accents': None,\n",
       " 'features__nlp_pipeline__vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'features__nlp_pipeline__vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'features__nlp_pipeline__vect__vocabulary': None,\n",
       " 'features__nlp_pipeline__tfidf__norm': 'l2',\n",
       " 'features__nlp_pipeline__tfidf__smooth_idf': True,\n",
       " 'features__nlp_pipeline__tfidf__sublinear_tf': False,\n",
       " 'features__nlp_pipeline__tfidf__use_idf': True,\n",
       " 'clf__estimator__bootstrap': True,\n",
       " 'clf__estimator__ccp_alpha': 0.0,\n",
       " 'clf__estimator__class_weight': None,\n",
       " 'clf__estimator__criterion': 'gini',\n",
       " 'clf__estimator__max_depth': None,\n",
       " 'clf__estimator__max_features': 'auto',\n",
       " 'clf__estimator__max_leaf_nodes': None,\n",
       " 'clf__estimator__max_samples': None,\n",
       " 'clf__estimator__min_impurity_decrease': 0.0,\n",
       " 'clf__estimator__min_impurity_split': None,\n",
       " 'clf__estimator__min_samples_leaf': 1,\n",
       " 'clf__estimator__min_samples_split': 2,\n",
       " 'clf__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__estimator__n_estimators': 1000,\n",
       " 'clf__estimator__n_jobs': None,\n",
       " 'clf__estimator__oob_score': False,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator__verbose': 0,\n",
       " 'clf__estimator__warm_start': False,\n",
       " 'clf__estimator': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=None, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       " 'clf__n_jobs': None}"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_randomfo.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "        'features__nlp_pipeline__vect__max_features': (5000, 10000),\n",
    "        'features__nlp_pipeline__tfidf__use_idf': (True, False),\n",
    "        'clf__estimator__min_samples_split': [2, 4],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_randomfo = GridSearchCV(pipeline_randomfo, param_grid=parameters, cv = 2, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = time.time()\n",
    "# fit the train data\n",
    "cv_randomfo.fit(X_train, y_train)\n",
    "# get the pred value\n",
    "y_pred_randomfo = cv_randomfo.predict(X_test)\n",
    "\n",
    "time_cost = time.time() - now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def load_data(database_filepath):\n",
    "    engine = create_engine('sqlite:///DisasterResponse.db')\n",
    "    df = pd.read_sql('SELECT * FROM message', engine)\n",
    "    X = df.message\n",
    "    y = df.iloc[:, 4:]\n",
    "    category_names = list(y.columns)\n",
    "    return X, y, category_names\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens\n",
    "\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    pass\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, Y_test, category_names):\n",
    "    pass\n",
    "\n",
    "\n",
    "def save_model(model, model_filepath):\n",
    "    pass\n",
    "\n",
    "\n",
    "def main():\n",
    "    if len(sys.argv) == 3:\n",
    "        database_filepath, model_filepath = sys.argv[1:]\n",
    "        print('Loading data...\\n    DATABASE: {}'.format(database_filepath))\n",
    "        X, Y, category_names = load_data(database_filepath)\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "        \n",
    "        print('Building model...')\n",
    "        model = build_model()\n",
    "        \n",
    "        print('Training model...')\n",
    "        model.fit(X_train, Y_train)\n",
    "        \n",
    "        print('Evaluating model...')\n",
    "        evaluate_model(model, X_test, Y_test, category_names)\n",
    "\n",
    "        print('Saving model...\\n    MODEL: {}'.format(model_filepath))\n",
    "        save_model(model, model_filepath)\n",
    "\n",
    "        print('Trained model saved!')\n",
    "\n",
    "    else:\n",
    "        print('Please provide the filepath of the disaster messages database '\\\n",
    "              'as the first argument and the filepath of the pickle file to '\\\n",
    "              'save the model to as the second argument. \\n\\nExample: python '\\\n",
    "              'train_classifier.py ../data/DisasterResponse.db classifier.pkl')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
